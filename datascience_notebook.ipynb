{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.dii.uchile.cl/wp-content/uploads/2021/06/Magi%CC%81ster-en-Ciencia-de-Datos.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MDS7202: Laboratorio de Programaci√≥n Cient√≠fica para Ciencia de Datos**\n",
    "\n",
    "### üë®‚Äçüè´üë©‚Äçüè´ Cuerpo Docente:\n",
    "\n",
    "- Profesor: Sebasti√°n Tinoco, Stefano Schiappacasse\n",
    "- Auxiliar: Melanie Pe√±a Torres, Valentina Rojas Osorio\n",
    "- Ayudante: Valentina Zu√±iga, √Ångelo Mu√±oz \n",
    "\n",
    "### üë®‚Äçüíªüë©‚Äçüíª Estudiantes:\n",
    "- Estudiante n¬∞1: Marcos Ignacio Huenchumil Illanes\n",
    "- Estudiante n¬∞2: Nicolas Fuenzalida Saez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Enunciado "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src='proyecto.png' style=\"border-radius: 12px\"> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el competitivo universo de las bebidas gaseosas, la empresa **SodAI Drinks ü•§** ha logrado destacarse por su creatividad, diversidad de productos y enfoque centrado en el cliente. Ofrece una extensa gama de bebidas carbonatadas que abarca distintos segmentos del mercado: desde productos premium en presentaciones sofisticadas, hasta gaseosas accesibles para el consumo masivo, disponibles en diversos tama√±os y tipos de envases. \n",
    "\n",
    "La compa√±√≠a opera en m√∫ltiples regiones y zonas, sirviendo a una variedad de puntos de venta que incluyen desde tiendas de conveniencia y minimarkets hasta el canal fr√≠o tradicional. Cada tipo de cliente tiene sus particularidades: algunos reciben entregas hasta 4 veces por semana, mientras que otros son visitados por la fuerza de ventas solo una vez semanalmente. Esta diversidad de perfiles representa tanto una oportunidad como un desaf√≠o comercial: \n",
    "\n",
    "\t\t¬øc√≥mo saber qu√© productos tienen m√°s chances de ser comprados por cada cliente en un momento dado?\n",
    "\n",
    "Con el objetivo de aumentar la facturaci√≥n de forma inteligente y mejorar la eficiencia de su estrategia de ventas, **SodAI Drinks** decide crear una nueva c√©lula interna de innovaci√≥n: el equipo **Deep Drinkers ü§ñ**, cuyo prop√≥sito es aplicar ciencia de datos para anticiparse a las necesidades del cliente y potenciar el negocio desde una perspectiva basada en informaci√≥n.\n",
    "\n",
    "El coraz√≥n de esta iniciativa es el desarrollo de un sistema predictivo personalizado para cada cliente. Para ello, **Deep Drinkers** convoca a un equipo de Data Scientists y especialistas en *machine learning* con una misi√≥n clara: ``construir un modelo predictivo que, cada semana, pueda estimar la probabilidad de compra de cada producto del portafolio para cada cliente activo.``\n",
    "\n",
    "El modelo deber√° tener en cuenta m√∫ltiples factores, incluyendo:\n",
    "- **Tipo de cliente**, ej. \"TIENDA DE CONVENIENCIA\", \"MINIMARKET\".\n",
    "- **Frecuencia de entregas y visitas**, indicadores del nivel de actividad comercial.\n",
    "- **Ubicaci√≥n geogr√°fica** (por regi√≥n y zona).\n",
    "- **Preferencias hist√≥ricas de consumo**, inferidas por patrones de compra anteriores.\n",
    "- **Caracter√≠sticas del producto**, como marca, categor√≠a, segmento, tipo de envase y tama√±o\n",
    "\n",
    "El objetivo final es que, **cada semana**, se genere una tabla de productos priorizados: para cada cliente, un listado de productos ordenado por su probabilidad estimada de compra. Esta informaci√≥n ser√° enviada al equipo comercial, que podr√° usarla en call center, para incrementar las chances de concretar ventas al ofrecer justo lo que el cliente probablemente quiere comprar.\n",
    "\n",
    "Este proyecto representa un cambio de paradigma en la forma en que **SodAI Drinks** gestiona su fuerza de ventas: de un enfoque reactivo y generalista, a uno proactivo, basado en datos y profundamente personalizado. As√≠, la empresa no solo espera aumentar su rentabilidad, sino tambi√©n construir relaciones m√°s s√≥lidas con sus clientes, ofreci√©ndoles recomendaciones m√°s relevantes y oportunas.\n",
    "\n",
    "Para lograr lo anterior, el equipo **Deep Drinkers** contar√° con los siguientes conjuntos de datos, junto a sus respectivos atributos:\n",
    "\n",
    "- **Datos transaccionales** (`transacciones.parquet`): contiene el historial de compras realizadas por los clientes.\n",
    "\t- `customer_id`: identificador √∫nico del cliente que realiz√≥ la compra.\n",
    "\t- `product_id`: identificador √∫nico del producto comprado.\n",
    "\t- `purchase_date`: fecha en que se realiz√≥ la transacci√≥n.\n",
    "\t- `order_id`: identificar de la orden de su pedido.\n",
    "\t- `payment`\tmonto total pagado por la transacci√≥n.\n",
    "\n",
    "- **Datos de clientes** (`clientes.parquet`): incluye las caracter√≠sticas de cada cliente.\n",
    "\t- `customer_id`: identificador √∫nico del cliente.\n",
    "\t- `region_id`: identificador de la regi√≥n geogr√°fica donde se encuentra el cliente.\n",
    "\t- `customer_type`: tipo de cliente seg√∫n el canal comercial, por ejemplo, ‚ÄúTIENDA DE CONVENIENCIA‚Äù.\n",
    "\t- `Y`: coordenada geogr√°fica de latitud.\n",
    "\t- `X`: coordenada geogr√°fica de longitud.\n",
    "\t- `num_deliver_per_week`: cantidad de entregas semanales que recibe el cliente.\n",
    "\t- `num_visit_per_week`: frecuencia de visitas de la fuerza de ventas por semana.\n",
    "\n",
    "- **Datos de productos** (`productos.parquet`): describe las caracter√≠sticas de los productos del portafolio.\n",
    "\t- `product_id`: identificador √∫nico del producto.\n",
    "\t- `brand`: marca comercial del producto.\n",
    "\t- `category`: categor√≠a general del producto, como ‚ÄúBEBIDAS CARBONATADAS‚Äù.\n",
    "\t- `sub_category`: subcategor√≠a dentro de la categor√≠a principal, por ejemplo, ‚ÄúGASEOSAS‚Äù.\n",
    "\t- `segment`: segmento de mercado al que pertenece el producto, como ‚ÄúPREMIUM‚Äù.\n",
    "\t- `package`: tipo de envase del producto.\n",
    "\t- `size`: tama√±o del producto en litros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Reglas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://media1.tenor.com/m/0Qtv_cQ4ITsAAAAd/necohaus-grey-name.gif\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "El proyecto consta de **dos entregas parciales** y una **entrega final** en donde la primera entrega la idea es poder reflejar lo aprendido durante la primera mitad del curso, que ser√° sobre los contenidos relacionados a *machine learning*, la segunda ser√° sobre los contenidos de la segunda mitad del curso relacionados a *MLOps* y por √∫ltimo la entrega final constar√° de dos partes, donde la primera ser√° relacionada con experimentaci√≥n sobre nuevos datasets que ser√°n disponibilizados durante las √∫ltimas semanas del curso de manera incremental y una segunda parte que ser√° el informe final escrito que deber√° explicar el desarrollo del proyecto completo, como tambien los resultados y an√°lisis de los experimentos realizados sobre los datasets incrementales. La idea es que todo el c√≥digo est√© desarrollado durante las primeras dos entregas y luego en la entrega final s√≥lo se ejecute el c√≥digo sobre nuevos conjuntos de datos.\n",
    "\n",
    "La idea de generar el proyecto por etapas es poder aliviar la carga de trabajo en las √∫ltimas semanas del semestre donde sabemos que est√°n muy cargado con entregas, pruebas y ex√°menes de otros ramos, y as√≠ garantizamos que habiendo la desarrollado las dos primeras entregas parciales, tendr√°n el grueso del proyecto listo para luego experimentar y documentar.\n",
    "\n",
    "---\n",
    "### **Fechas de entrega**\n",
    "- **Entrega parcial 1**: 14 de Mayo\n",
    "- **Entrega parcial 2**: Por definir\n",
    "- **Entrega final**: Por definir\n",
    "\n",
    "---\n",
    "\n",
    "### **Requisitos del proyecto**\n",
    "- **Grupos**: Formar equipos de **2 personas**. No se aceptar√°n trabajos individuales o grupos con m√°s integrantes.\n",
    "- **Consultas**: Cualquier duda fuera del horario de clases debe ser planteada en el foro correspondiente. Los mensajes enviados al equipo docente ser√°n respondidos √∫nicamente por este medio. Por favor, revisen las respuestas anteriores en el foro antes de realizar nuevas consultas.\n",
    "- **Plagio**: La copia o reutilizaci√≥n no autorizada de trabajos de otros grupos est√° **estrictamente prohibida**. El incumplimiento de esta norma implicar√° la anulaci√≥n inmediata del proyecto y una posible sanci√≥n acad√©mica.\n",
    "- **Material permitido**: Pueden usar cualquier material del curso, ya sea notas, lecturas, c√≥digos, o referencias proporcionadas por los docentes, que consideren √∫til para el desarrollo del proyecto.\n",
    "\n",
    "---\n",
    "\n",
    "### **Entregables y etapas**\n",
    "\n",
    "#### **1. Entrega Parcial 1**  \n",
    "- Dispondr√°n de los archivos de datos **productos.parquet**, **clientes.parquet** y **transacciones.parquet** para el modelamiento inicial.  \n",
    "- Utilizar√°n estos archivos para desarrollar lo solicitado para la entrega 1. \n",
    "- En esta etapa, se espera que apliquen todos los conocimientos aprendidos durante la primera parte del curso relacionados con *machine learning*.\n",
    "- **Informe**: No se exige un avance del informe en esta etapa, s√≥lo un notebook con su desarrollo actual, pero se **recomienda comenzar** a redactar el informe final en paralelo para disminuir la carga acad√©mica en las etapas posteriores.  \n",
    "\n",
    "#### **2. Entrega Parcial 2**  \n",
    "- En esta entrega, deber√°n aplicar los conocimientos aprendidos durante la segunda mitad del curso sobre *MLOps*  \n",
    "- Se espera que implementen estos conocimientos para desplegar su modelo elegido en la primera entrega y crear *pipelines* automatizados que simulen un entorno productivo.\n",
    "- **Informe**: similar a la primera etapa, no se exige un avance del informe, pero se **recomienda avanzar con su redacci√≥n** para evitar una acumulaci√≥n de trabajo en la etapa final.  \n",
    "\n",
    "#### **3. Entrega Final**  \n",
    "- En la entrega final, deber√°n realizar dos etapas:\n",
    "\t- La primera etapa es sobre experimentaci√≥n utilizando datasets incrementales que se ir√°n disponibilizando de manera parcial, para que vayan generando predicciones con su modelo ya desplegado. El objetivo de esta etapa es poder testear su soluci√≥n *end-to-end* y que vayan analizando los resultados obtenidos a medida que se van agregando m√°s datos.\n",
    "\t- La segunda etapa consiste en redactar un informe final que deber√° explicar el desarrollo completo de tu proyecto y un an√°lisis profundo de sus resultados de experimentaci√≥n. Este informe debera incluir a lo menos las siguientes secciones:\n",
    "\t\t- An√°lisis exploratorio de datos  \n",
    "\t\t- Metodolog√≠a aplicada  \n",
    "\t\t- Selecci√≥n y entrenamiento de modelos  \n",
    "\t\t- Evaluaci√≥n de resultados  \n",
    "\t\t- Optimizaci√≥n de modelos\n",
    "\t\t- Interpretabilidad\n",
    "\t\t- Re-entrenamiento\n",
    "\t\t- Tracking con MLFlow\n",
    "\t\t- Creaci√≥n de la aplicaci√≥n web con Gradio y FastAPI\n",
    "\n",
    "Es **altamente recomendable** ir redactando el informe en paralelo al desarrollo de los modelos para garantizar que toda la informaci√≥n relevante quede documentada adecuadamente.  \n",
    "\n",
    "### Nota Final\n",
    "\n",
    "La calificaci√≥n final de su proyecto se calcular√° utilizando la siguiente ponderaci√≥n: \n",
    "\n",
    "$$Nota Final = 0.30 * EntregaParcial1 + 0.40 * EntregaParcial2 + 0.30 * EntregaFinal$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Instrucciones importantes**\n",
    "\n",
    "1. **Formato del informe**:  \n",
    "   - El informe debe estar integrado dentro de un **Jupyter Notebook**. No es necesario subirlo a una plataforma externa, pero debe cumplir con los siguientes requisitos:  \n",
    "     - Estructura clara y ordenada.  \n",
    "     - C√≥digo acompa√±ado de explicaciones detalladas.  \n",
    "     - Resultados presentados de forma visual y anal√≠tica.  \n",
    "\n",
    "2. **Descuento por informes deficientes**:  \n",
    "   - Cualquier secci√≥n del informe que no tenga una explicaci√≥n adecuada o no respete el formato ser√° penalizada con un descuento en la nota. Esto incluye c√≥digo sin comentarios o an√°lisis que no sean coherentes con los resultados presentados.\n",
    "   - Comentarios sin formatear de ChatGPT o herramientas similares ser√°n penalizados (e.g: \"Inserta tu modelo ac√°\", etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¨ Entrega Parcial 1 (30% del Proyecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Abstract [0.25 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.redd.it/h5ptnsyabqvd1.gif\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, deben redactar un Abstract claro y conciso para su proyecto. El Abstract debe responder a las siguientes preguntas clave:\n",
    "\n",
    "- **Descripci√≥n del problema**: ¬øCu√°l es el objetivo del proyecto? ¬øQu√© se intenta predecir o analizar?\n",
    "- **Datos de entrada**: ¬øQu√© datos tienen disponibles? ¬øCu√°les son sus principales caracter√≠sticas?\n",
    "- **M√©trica de evaluaci√≥n**: ¬øC√≥mo medir√°n el desempe√±o de sus modelos? Expliquen por qu√© eligieron esta m√©trica bas√°ndose en el an√°lisis exploratorio de los datos.\n",
    "- **Modelos y transformaciones**: ¬øQu√© modelos utilizar√°n y por qu√©? ¬øQu√© transformaciones o preprocesamientos aplicaron a los datos?\n",
    "- **Resultados generales**: ¬øEl modelo final cumpli√≥ con los objetivos del proyecto? ¬øCu√°les fueron las conclusiones m√°s importantes?\n",
    "\n",
    "**Importante**: Escriban esto despues de haber resuelto el resto de la tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Descripci√≥n del problema**\n",
    "- ¬øCu√°l es el objetivo del proyecto? El objetivo del proyecto, como se mencion√≥ anteriormente es: construir un modelo predictivo que, cada semana, pueda estimar la probabilidad de compra de cada producto del portafolio para cada cliente activo. La idea es, una vez construido el modelo, darle esta informaci√≥n al equipo de Marketing para que aprovechen negocios nuevos u ofertas a clientes.\n",
    "- ¬øQu√© se intenta predecir o analizar? Se intenta predecir el comportamiento que tienen los clientes activos con la informaci√≥n de los 3 datasets para predecir qu√© son m√°s propensos a comprar, dado su historial de compras y comportamientos anteriores.\n",
    "\n",
    "2. **Datos de entrada**\n",
    "- ¬øQu√© datos tienen disponibles? Seg√∫n se nos menciona anteriormente, los datos que tenemos disponibles son de 3 datasets: Datos transaccionales (transacciones.parquet): contiene el historial de compras realizadas por los clientes, Datos de clientes (clientes.parquet): incluye las caracter√≠sticas de cada cliente y Datos de productos (productos.parquet): describe las caracter√≠sticas de los productos del portafolio.\n",
    "- ¬øCu√°les son sus principales caracter√≠sticas? Importando los datasets, se puede observar que el dataset:\n",
    "\n",
    "    a. \"Transacciones\" tiene 5 columnas: \"customer_id\" que es la llave para cada cliente, \"product_id\" que es la llave de cada producto, \"order_id\" que es la llave de cada orden realizada, \"purchase_date\" que muestra la fecha de la compra y \"items\" que muestra la cantidad de bultos comprados por el cliente en esa transacci√≥n (existen valores negativos porque ciertas transacciones realizadas, luego se cancelan por alg√∫n motivo particular), adem√°s no existe la columna \"payment\".\n",
    "\n",
    "    b. \"Clientes\" tiene 8 columnas: \"customer_id\" ya explicada, \"region_id\" la llave de la region, \"zone_id\" llave de la zona (una regi√≥n puede contener varias zonas), \"customer_type\" que tiene 7 tipos de clientes (abarrotes, restaurant, supermercado, etc.), \"Y\" y \"X\" presentan las coordenadas, \"num_deliver_per_week\" y \"num_visit_per_week\" la cantidad de entregas y visitas semanalmente. \n",
    "\n",
    "    c. \"Productos tiene 7 columnas: \"product_id\" visto antes, \"brand\" es la marca del producto, \"category\" es categor√≠a general de producto (carbonatada o no carbonatada), \"sub_category\" puede ser gaseosas, aguas saborizadas o jugos, \"segment\" es el segmento al que pertenece el producto, \"package\" es botella, lata, tetra o keg y \"size\" es el tama√±o del producto en litros.\n",
    "\n",
    "3. **M√©trica de evaluaci√≥n**\n",
    "- ¬øC√≥mo medir√°n el desempe√±o de sus modelos? Expliquen por qu√© eligieron esta m√©trica bas√°ndose en el an√°lisis exploratorio de los datos.\n",
    "\n",
    "4. **Modelos y transformaciones**\n",
    "- ¬øQu√© modelos utilizar√°n y por qu√©?\n",
    "- ¬øQu√© transformaciones o preprocesamientos aplicaron a los datos?\n",
    "\n",
    "5. **Resultados generales**\n",
    "- ¬øEl modelo final cumpli√≥ con los objetivos del proyecto?\n",
    "- ¬øCu√°les fueron las conclusiones m√°s importantes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Pre-procesamiento [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media0.giphy.com/media/10zsjaH4g0GgmY/giphy.gif?cid=6c09b9523xtlunksc9amikw09zk1bmiqwjqnt70ae82rk877&ep=v1_gifs_search&rid=giphy.gif&ct=g\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como en muchos otros problemas de negocio, los datos probablemente deben ser pre procesados antes de aplicar cualquier t√©cnica de anal√≠tica. Bajo esa premisa, en esta secci√≥n deben desarrollar c√≥digo que les permita **preparar los datos** de tal forma que les permita resolver el problema planteado. Para esto, pueden aplicar procesamientos como:\n",
    "\n",
    "- Transformaciones de tipo de dato (str, int, etc)\n",
    "- Cruce de informaci√≥n\n",
    "- Eliminaci√≥n de duplicados\n",
    "- Filtros de fila y/o columnas\n",
    "\n",
    "*Hint: ¬øQu√© forma deber√≠a tener la data para resolver un problema de aprendizaje supervisado?*\n",
    "\n",
    "Todo proceso llevado a cabo debe estar bien documentado y justificado en el informe, explicando el por qu√© se decidi√≥ realizar en funcion de los datos presentados y los objetivos planteados del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import set_config\n",
    "\n",
    "# se configura el modo de salida de los DataFrames\n",
    "set_config(transform_output=\"pandas\")\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se cargan los datos\n",
    "df_transacciones = pd.read_parquet(\"data/transacciones.parquet\")\n",
    "df_clientes = pd.read_parquet(\"data/clientes.parquet\")\n",
    "df_productos =  pd.read_parquet(\"data/productos.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "#funcion auxiliar para el preprocesamiento\n",
    "class IQR(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lambda_=1.5):\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        q1 = X.quantile(0.25)\n",
    "        q3 = X.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        self.lower_bound_ = q1 - self.lambda_ * iqr\n",
    "        self.upper_bound_ = q3 + self.lambda_ * iqr\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in X.columns:\n",
    "            X_transformed.loc[(X[col] < self.lower_bound_[col]) | (X[col] > self.upper_bound_[col]), col] = np.nan\n",
    "        return X_transformed\n",
    "\n",
    "    def count_outliers(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = X.to_frame()\n",
    "        resumen = []\n",
    "        for col in X.columns:\n",
    "            if col in self.lower_bound_:\n",
    "                outliers = (X[col] < self.lower_bound_[col]) | (X[col] > self.upper_bound_[col])\n",
    "                resumen.append({\n",
    "                    'columna': col,\n",
    "                    'outliers': outliers.sum(),\n",
    "                    'porcentaje': 100 * outliers.sum() / len(X)\n",
    "                })\n",
    "        return pd.DataFrame(resumen).sort_values(by='porcentaje', ascending=False)\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return input_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# funcion auxiliar para graficar boxplots\n",
    "def bboxplot(series: pd.Series, title: str = \"\", use_deciles: bool = False, k: float = 1.5):\n",
    "    \"\"\"\n",
    "    Genera un boxplot usando seaborn a partir de una pd.Series.\n",
    "    Puede usar deciles (D1, D9) en lugar del IQR cl√°sico.\n",
    "\n",
    "    Par√°metros:\n",
    "    - series: pd.Series con los datos num√©ricos.\n",
    "    - title: str, t√≠tulo del gr√°fico.\n",
    "    - use_deciles: bool, si True usa D1 y D9 en vez de Q1 y Q3.\n",
    "    - k: float, coeficiente para definir outliers con deciles.\n",
    "    \"\"\"\n",
    "    if not isinstance(series, pd.Series):\n",
    "        raise TypeError(\"El par√°metro debe ser una pd.Series.\")\n",
    "\n",
    "    if use_deciles:\n",
    "        d1 = series.quantile(0.25) # primer decil\n",
    "        d9 = series.quantile(0.75) # noveno decil\n",
    "        spread = d9 - d1\n",
    "        lower_bound = d1 - k * spread\n",
    "        upper_bound = d9 + k * spread\n",
    "        outliers = series[(series < lower_bound) | (series > upper_bound)].count()\n",
    "        computed_title = (\n",
    "            f\"{series.name} \"\n",
    "            f\"D1={d1:.2f}, D9={d9:.2f}, Rango={spread:.2f}, Outliers={outliers}\"\n",
    "        )\n",
    "    else:\n",
    "        q1 = series.quantile(0.25)\n",
    "        q3 = series.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - k * iqr\n",
    "        upper_bound = q3 + k * iqr\n",
    "        outliers = series[(series < lower_bound) | (series > upper_bound)].count()\n",
    "       \n",
    "        computed_title = (\n",
    "            f\"{series.name} \"\n",
    "            f\"Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}, Outliers={outliers}\"\n",
    "        )\n",
    "\n",
    "    porcentage = f\" ({outliers / len(series) * 100:.2f}%)\" if outliers else \"\"\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    sns.boxplot(x=series, color='blue')\n",
    "    plt.axvline(lower_bound, color='blue', linestyle='--', label='L√≠mite inferior', linewidth=1)\n",
    "    plt.axvline(upper_bound, color='blue', linestyle='--', label='L√≠mite superior', linewidth=1)\n",
    "    plt.title(title + '\\n' + computed_title + porcentage or computed_title)\n",
    "    plt.xlabel(series.name or \"Valor\")\n",
    "    plt.grid(True, axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# funci√≥n auxiliar para graficar barplots\n",
    "def barplot(serie, titulo='Distribuci√≥n de categor√≠as', xlabel='Categor√≠a', ylabel='Frecuencia', rot=45):\n",
    "    conteo = serie.value_counts().sort_index()\n",
    "    df_plot = pd.DataFrame({xlabel: conteo.index, ylabel: conteo.values})\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(data=df_plot, x=xlabel, y=ylabel)\n",
    "    plt.title(f\"{titulo}: {serie.name}\")\n",
    "    plt.xticks(rotation=rot)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataframe transacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se visualizan algunos datos\n",
    "df_transacciones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se verifica el tipo de datos y la existencia de valores nulos -> sin valores nulos\n",
    "df_transacciones.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumen estad√≠stico de las variables num√©ricas. sirve para ver si hay outliers\n",
    "df_transacciones.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformaciones de tipo de datos: los tipos de datos estan bien definidos\n",
    "df_transacciones.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicados\n",
    "df_transacciones_without_duplicated = df_transacciones.drop_duplicates(subset=['customer_id', 'product_id', 'purchase_date', 'items'])\n",
    "print(f\"se eliminan registro duplicados: {df_transacciones.shape[0] - df_transacciones_without_duplicated.shape[0]}\")\n",
    "print(f\"porcentaje: {(100 * (df_transacciones.shape[0] - df_transacciones_without_duplicated.shape[0]) / df_transacciones.shape[0]):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataframe ``transacciones`` los tipos de datos son consistentes con los valores ingresados, por lo que no fue necesario realizar transformaci√≥n a los tipos. \n",
    "\n",
    "Para analizar la existencia de registros duplicados, se elimin√≥ la clave primaria `order_id` del conjunto de columnas, permitiendo as√≠ detectar registros que son id√©nticos en todos los dem√°s atributos. Se encuentran 885 registros duplicados, lo que representa aproximadamente el 0.35% del total de transacciones. Al ser una cantidad no significativa se decide precindir de estos datos.\n",
    "\n",
    "En cuanto a la utilidad de las features:\n",
    "- `order_id`: indetificador √∫nico de cada transacci√≥n (por eliminar en el df_completed)\n",
    "- `customer_id`, `product_id`: permiten realizar el cruce de informaci√≥n con las otras dataframes\n",
    "- `purchase_date`: aporta informaci√≥n temporal que puede ser √∫til para identificar patrones de comportamiento o estacionalidades\n",
    "- `items`: indica la cantidad de productos comprados en cada transacciones \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transacciones_cleaned = df_transacciones_without_duplicated.copy()\n",
    "df_transacciones_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers\n",
    "bboxplot(df_transacciones_cleaned['items'], title='Distribuci√≥n de items', use_deciles=False, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_shopping = {\n",
    "    \"devolucion\": (df_transacciones_without_duplicated['items'] < 0).sum(),\n",
    "    \"compra\": (df_transacciones_without_duplicated['items'] >= 0).sum()\n",
    "}\n",
    "\n",
    "porcentaje_devoluciones = round(\n",
    "    dir_shopping['devolucion'] / (dir_shopping['compra'] + dir_shopping['devolucion']) * 100,\n",
    "    2\n",
    ")\n",
    "print(f\"Porcentaje de devoluciones: {porcentaje_devoluciones}%, devolucion: {dir_shopping['devolucion']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar row con items negativos\n",
    "df_transacciones_cleaned = df_transacciones_cleaned[df_transacciones_cleaned['items'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataframe clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se verifica el tipo de datos y la existencia de valores nulos -> existe un valor nulo para la variable 'X'\n",
    "df_clientes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se corrobora la existencia de valores nulos\n",
    "df_clientes.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se elimina el registor con X nulo\n",
    "df_clientes = df_clientes[df_clientes['X'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se convierte la columna de tipo 'object' a 'category' para optimizar el uso de memoria\n",
    "df_clientes = df_clientes.apply(\n",
    "    lambda col: col.astype('category') if col.dtype == 'object' else col\n",
    ")\n",
    "\n",
    "# se tienen las variables region_id y zone_id como foreign_keys de una tabla que no se tiene, pero su naturaleza es categ√≥rica\n",
    "df_clientes['region_id'] = df_clientes['region_id'].astype('category')\n",
    "df_clientes['zone_id'] = df_clientes['zone_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# las variables X, Y, num_Deliver_per_week, num_visit_per_week deben serr normalizadas\n",
    "df_clientes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicados\n",
    "df_clientes_without_duplicated = df_clientes.drop_duplicates(\n",
    "    subset=df_clientes.drop(columns=['customer_id']).columns\n",
    ")\n",
    "\n",
    "# Calcular y mostrar resultados\n",
    "num_duplicados = df_clientes.shape[0] - df_clientes_without_duplicated.shape[0]\n",
    "porcentaje_duplicados = 100 * num_duplicados / df_clientes.shape[0]\n",
    "\n",
    "print(f\"Se eliminan registros duplicados: {num_duplicados}\")\n",
    "print(f\"Porcentaje: {porcentaje_duplicados:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataframe ``clientes`` la feature ``customer_type`` se convierte desde tipo object a category, este cambio mejora el rendimiento en el uso de memoria. Se tiene las variables `region_id`, `zone_id` que son foreign_key de una tabla no disponible, aunque por su naturaleza estos deben corresponder a datos categ√≥ricos.\n",
    "\n",
    "Para analizar la existencia de registros duplicados, se elimin√≥ la clave primaria `customer_id` del conjunto de columnas, permitiendo as√≠ detectar registros que son id√©nticos en todos los dem√°s atributos. Existen registros que solo varian levemente en los decimales de las coordenadas X e Y, apesar de esto no se puede asegurar que corresponde a un mismo cliente, por tanto no se encuentran registros duplicados. Se encuentra que existe un valor nulo en la feauture `X`, al ser poco significativo se elimna dicho registro.\n",
    "\n",
    "En cuanto a la utilidad de las features se menciona:\n",
    "- `region_id`, `zone_id`: se utilizan como parte de la informaci√≥n geografica requerida en el enunciado. (\"Ubicaci√≥n geogr√°fica\")\n",
    "- `customer_type`: aporta informaci√≥n √∫til sobre el tipo de cliente (\"Tipo de Cliente\")\n",
    "- `X`, `Y`: puede ser informaci√≥n √∫til para diferenciar clientes en diferentes localidades (\"Ubicaci√≥n geogr√°fica\") \n",
    "- `num_deliver_per_week`, `num_visit_per_week`: Ambas variables cuantifican la frecuencia de interacci√≥n con el cliente (entregas y visitas, respectivamente) y se consideran esenciales para el an√°lisis. (\"Frecuencia de entregas y visitas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes_cleaned = df_clientes_without_duplicated.copy()\n",
    "df_clientes_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataframe productos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se verifica el tipo de datos y la existencia de valores nulos -> sin valores nulos\n",
    "df_productos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se convierte la columna de tipo 'object' a 'category' para optimizar el uso de memoria\n",
    "df_productos = df_productos.apply(\n",
    "    lambda col: col.astype('category') if col.dtype == 'object' else col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos_without_duplicated = df_productos.drop_duplicates(subset=df_productos.drop(columns=['product_id']).columns)\n",
    "print(f\"se eliminan registro duplicados: {df_productos.shape[0] - df_productos_without_duplicated.shape[0]}. porcentaje: {(100 * (df_productos.shape[0] - df_productos_without_duplicated.shape[0]) / df_productos.shape[0]):.2f}%.\")\n",
    "print(f\"cantidad de productos finales: {df_productos_without_duplicated.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el dataframe ``productos`` la feature `brand`, `category`, `sub_category`, `segment`, `package` se convierte desde tipo object a category, este cambio mejora el rendimiento en el uso de memoria.\n",
    "\n",
    "Para analizar la existencia de registros duplicados, se elimin√≥ la clave primaria `product_id` del conjunto de columnas, permitiendo as√≠ detectar registros que son id√©nticos en todos los dem√°s atributos. Se encuentran 776 registros duplicados, lo que equivale a aproximadamente un 80% de los registros de productos. No se encuentras valores nulos.\n",
    "\n",
    "En cuanto a la utilidad de las features se menciona:\n",
    "- product_id: Es una clave primaria y se utiliza para enlazar este DataFrame con otros, como transacciones. Aunque muchos IDs est√°n duplicados en t√©rminos de atributos, deben conservarse para mantener integridad referencial.\n",
    "\n",
    "- brand, category, sub_category, segment, package: Estas variables categ√≥ricas describen distintas dimensiones del producto. Son relevantes para an√°lisis de mercado, segmentaci√≥n, y modelado de comportamiento de compra.\n",
    "\n",
    "- size: Representa una medida f√≠sica o de presentaci√≥n del producto. Es √∫til para agrupar productos similares, ajustar precios o analizar log√≠stica de distribuci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos_cleaned = df_productos_without_duplicated.copy()\n",
    "len(df_productos_cleaned['product_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapear los ids de productos duplicados a un id √∫nico\n",
    "cols_without_id = df_productos.columns.difference(['product_id'])\n",
    "df_productos['_dedup_key'] = df_productos[cols_without_id.tolist()].apply(lambda row: tuple(row), axis=1)\n",
    "\n",
    "dedup_reference = (\n",
    "    df_productos.drop_duplicates(subset='_dedup_key')[['_dedup_key', 'product_id']]\n",
    "    .rename(columns={'product_id': 'kept_product_id'})\n",
    ")\n",
    "\n",
    "df_mapeo_ids = df_productos.merge(dedup_reference, on='_dedup_key', how='left')\n",
    "df_mapeo_ids = df_mapeo_ids[['product_id', 'kept_product_id']]\n",
    "print(f\"cantidad de productos unicos: {df_mapeo_ids['kept_product_id'].nunique()}\")\n",
    "# de los 971 productos originales, existen 195 productos diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe 195 productos identificados de forma unica en el dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cruce de informaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_productos_cleaned['product_id'].unique()) # si bien hay 195 ids de productos diferentes\n",
    "# existen ids diferentes que hacen referencia al mismo producto. Por lo que la cantidad de productos realmente unicos es menor a 195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort\n",
    "df_productos_cleaned['product_id'].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transacciones_cleaned['product_id'].nunique() # la tabla de transacciones considera 114 product_id unicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapeo_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge productos\n",
    "df_merge_transacciones_productos = pd.merge(df_transacciones_cleaned, df_productos, on='product_id', how='left')\n",
    "# mapear los ids de productos de transacciones a un id √∫nico\n",
    "df_merge_transacciones_productos = df_merge_transacciones_productos.merge(df_mapeo_ids, left_on='product_id', right_on='product_id', how='left')\n",
    "# merge con los clientes\n",
    "df_merge_transacciones_productos_clientes = pd.merge(\n",
    "    df_merge_transacciones_productos,\n",
    "    df_clientes_cleaned,\n",
    "    left_on='customer_id',\n",
    "    right_on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_completed_true = df_merge_transacciones_productos_clientes.drop(columns=['_dedup_key'])\n",
    "\n",
    "#cambiar nombre variable\n",
    "df_completed_true = df_completed_true.drop(columns=['product_id']) # se elimina la columna original de prodcutos disponibles\n",
    "df_completed_true = df_completed_true.rename(columns={'kept_product_id': 'product_id'}) # se mapea el id de producto a un id √∫nico\n",
    "\n",
    "print(f\"df_completed: {df_completed_true.shape}, cantidad de productos unicos: {df_completed_true['product_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien existen 195 productos √∫nicos en el dataframe `df_productos` desde los 971 productos registrados, existe un conjunto de 114 product_id en el dataframe `df_transacciones` que pertecenen dentor de los 971 productos posibles, pero no necesariamente estan mapeados dentro de los cantidad de ids unicso registrados (en los 195). Pueden existir por ejemplo dos IDs entre los 114 que correspondan a dos elementos entre los 971 productos, pero que correspondan a registro duplicados.\n",
    "\n",
    "Por lo anterior se realiza el cruce con la tabla `df_mapeo_ids` que para todos los elementos duplicados los lleva hacia un mismo ID. De esta forma, la cantidad total de productos registrados de forma unica  son 56."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# funcion auxiliar para graficar barplots variables num√©ricas\n",
    "def plot_int_columns_bar_charts(df, max_unique_values=4):\n",
    "    int_columns = df.select_dtypes(include=['int64', 'int32']).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        unique_vals = df[col].nunique()\n",
    "        if unique_vals <= max_unique_values:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            df[col].value_counts().sort_index().plot(kind='bar')\n",
    "            plt.title(f'Distribuci√≥n de {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frecuencia')\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(df_clientes['num_visit_per_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(df_clientes['customer_type'])\n",
    "print(df_clientes['customer_type'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(df_clientes['num_deliver_per_week'])\n",
    "print(df_clientes['num_deliver_per_week'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint: ¬øQu√© forma deber√≠a tener la data para resolver un problema de aprendizaje supervisado?*\n",
    "\n",
    "Un problema de de aprendizaje supervisado implica relacionar un conjunto de features (M) con una etiqueta de predicci√≥n (y).\n",
    "\n",
    "Se debe recordar que la pregunta de investigaci√≥n es \"Estime, cada semana, la probabilidad de compra de cada producto del portafolio para cada cliente activo\". Es decir, nuestro dataset debe tener registros que relaciones al cliente X con el producto Y en la semana Z para estimar una variable binaria y, que corresponde a si la tupla (X,Y,Z) es positiva (compra el producto) o cero (no compra el producto). Por tanto es necesario estructurar el dataset para que cada registro represente una predicci√≥n binaria sobre el conjutno de reglas (X, Y, Z).\n",
    "\n",
    "\n",
    "Adicionalmente para considerar la temporalidad de los datos, se debe considerar una ventana deslizante que permita predecir el comportamiento de compra en la semana Z, considerando las compras realizadas en semanas anteriores. Para esto se debe crear una nueva feature que represente la semana de compra y otra que represente la semana de predicci√≥n. El problema finalmente, corresponde a un clasificador binario que permita predecir la probabilidad de compra de cada producto en la semana Z, dado el historial de compras en semanas anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "# import pandas as pd\n",
    "\n",
    "# # Pipelines separados\n",
    "# num_pipeline = Pipeline([\n",
    "#     ('outlier', IQR(lambda_=1)),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# cat_pipeline = Pipeline([\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "# ])\n",
    "\n",
    "# # ColumnTransformer\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', num_pipeline, ['size']),\n",
    "#     ('cat', cat_pipeline, ['brand', 'category', 'sub_category', 'segment', 'package']),\n",
    "# ], remainder='passthrough',\n",
    "#    verbose_feature_names_out=False\n",
    "# ).set_output(transform='pandas')\n",
    "\n",
    "# # Aplica el preprocesador\n",
    "# X = df_productos\n",
    "# df_productos_transformed = preprocessor.fit_transform(X)\n",
    "# df_productos_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üìå EDA [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExbHZ6aGdkd21tYTI3cW8zYWhyYW5wdGlyb2s3MmRzeTV0dzQ1NWlueiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3k1hJubTtOAKPKx4k3/giphy.gif\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, se debe realizar un an√°lisis exploratorio de los datos para comprender su estructura, detectar posibles problemas y obtener informaci√≥n relevante para el entrenamiento de los modelos. La idea es que puedan detectar **patrones en los datos** que les permitan resolver el problema con mayor facilidad.\n",
    "\n",
    "Se deben responder preguntas a partir de lo que puedan visualizar/obtener, por ejemplo:\n",
    "\n",
    "- Clientes y productos\n",
    "\n",
    "    - ¬øCu√°ntos clientes √∫nicos hay en el dataset?\n",
    "\n",
    "    - ¬øCu√°ntos productos √∫nicos se encuentran en los datos?\n",
    "\n",
    "- Periodo y frecuencia\n",
    "\n",
    "    - ¬øDe qu√© periodo es la informaci√≥n disponible?\n",
    "\n",
    "    - ¬øCu√°l es la frecuencia de los registros (diaria, semanal, mensual, etc.)?\n",
    "\n",
    "- Calidad de los datos\n",
    "\n",
    "    - ¬øExisten valores nulos en el dataset? ¬øCu√°ntos? ¬øC√≥mo se pueden tratar?\n",
    "\n",
    "    - ¬øHay datos raros, como cantidades negativas o inconsistencias? Genere tests de validaci√≥n para identificar estos problemas.\n",
    "\n",
    "- Patrones de compra\n",
    "\n",
    "    - ¬øCu√°ntos productos compra en promedio cada cliente semana a semana?\n",
    "\n",
    "    - ¬øCu√°ntas transacciones ha realizado cada cliente?\n",
    "\n",
    "    - ¬øCu√°l es el periodo de recompra promedio de cada SKU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### - Clientes y productos\n",
    "- ¬øCu√°ntos clientes √∫nicos hay en el dataset?\n",
    "    Se tienen 1568 clientes registrados, donde 1490 han tenido transacciones durante el periodo del 2024\n",
    "- ¬øCu√°ntos productos √∫nicos se encuentran en los datos? Hay 971 productos en total, pero solo se han comercializado 114. Como se menciono al estructuras el dataframe `df_completed` dentor de los 114 IDs registrados, realmente se hace referencia a 56 productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Clientes disponibles en el dataframe df_clientes: {df_clientes_cleaned.shape[0]},  \\\n",
    "    Clientes activos en df_transacciones: {df_completed_true[\"customer_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Productos registrados en df_productos: {df_productos_cleaned.shape[0]}. \\\n",
    "      productos activos en df_transacciones: {len(df_completed_true['product_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### - Periodo y frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se ordenan todas las fechas de transaccionees y se eliminan duplicados para capturar la frecuencia temporal\n",
    "sorted_dates = df_completed_true['purchase_date'].sort_values().drop_duplicates()\n",
    "frequency = pd.infer_freq(sorted_dates)\n",
    "\n",
    "# iniciar la frecuencia temporal y terminar de los registros\n",
    "start_date = sorted_dates.iloc[0]\n",
    "end_date = sorted_dates.iloc[-1]\n",
    "print(f\"Frecuencia temporal inferida: {frequency}\")\n",
    "print(f\"Fecha de inicio: {start_date}\")\n",
    "print(f\"Fecha de fin: {end_date}\")\n",
    "print(f\"Cantidad de fechas √∫nicas: {len(sorted_dates)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- ¬øDe qu√© periodo es la informaci√≥n disponible? \n",
    "\n",
    "    Se tienen registros de todos los d√≠as del a√±o 2024, desde 1ero de enero del 2024 hasta el 31 de dicienble del 2024 (366 d√≠as).\n",
    "\n",
    "- ¬øCu√°l es la frecuencia de los registros (diaria, semanal, mensual, etc.)?\n",
    "\n",
    "    Para capturar la frecuencia temporal se ordena por la feature ``purchase_date`` y se contabilizan los elementos unicos, eliminando los duplicados. El resultado son 366 registros en el a√±o 2024, lo cual se confirma al usar la funcion infer_freq de que la frecuencia es diaria. Esto plantea la necesidad de aplicar una transformacion de datos dado que el objetivo del modelo es realizar predicciones semanales por cliente-producto. Se debe agrupar los datos por semana calendario para alinear con el horizonte de predicciones del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### - Calidad de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed_true.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_shopping = {\n",
    "    \"devolucion\": (df_transacciones_without_duplicated['items'] < 0).sum(),\n",
    "    \"compra\": (df_transacciones_without_duplicated['items'] >= 0).sum()\n",
    "}\n",
    "\n",
    "porcentaje_devoluciones = round(\n",
    "    dir_shopping['devolucion'] / (dir_shopping['compra'] + dir_shopping['devolucion']) * 100,\n",
    "    2\n",
    ")\n",
    "print(f\"Porcentaje de devoluciones: {porcentaje_devoluciones}%, devolucion: {dir_shopping['devolucion']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxplot(df_completed_true['items'], title='Distribuci√≥n de items', use_deciles=False, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxplot(df_completed_true['X'], title='Distribuci√≥n de X', use_deciles=False, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxplot(df_completed_true['Y'], title='Distribuci√≥n de X', use_deciles=False, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- ¬øExisten valores nulos en el dataset? ¬øCu√°ntos? ¬øC√≥mo se pueden tratar? \n",
    "\n",
    "    Durante el an√°lisis exploratorio, se utiliz√≥ `df.describe()` junto con `df.isnull().sum()` para evaluar la presencia de valores nulos. Se identifica un √∫nico valor nulo en la feature X (coordenada de localizaci√≥n del cliente) en el dataFrame `df_clientes.`\n",
    "\n",
    "\n",
    "- ¬øHay datos raros, como cantidades negativas o inconsistencias? Genere tests de validaci√≥n para identificar estos problemas.\n",
    "\n",
    "    Si en el dataset `df_transacciones` la feauture `items` que representa \"cantidad de unidades de venta para el producto product_id\" toma valores negativos en el 3.29% de las transacciones. Este signo negativo indica transacciones anuladas o devoluciones, como productos no entregados, pedidos cancelados o errores en el proceso log√≠stico. En particular, 8346 registros est√°n etiquetados como ‚Äúproducto no entregado‚Äù.\n",
    "    \n",
    "    Tambi√©n se destacan la alta presencia de valores at√≠picos (outliers) como: X, Y, Items en `df_completed_true`, estos valores deben ser tratados mediante eliminacion u imputaci√≥n para no sesgar el rendimiento del modelo. Por √∫ltimo desde `df.describe()` es notorio que las features num√©ricas se encuentran en diferentes escalas, por lo que es necesario aplicar alguna t√©cnica de normalizaci√≥n, lo cual depender√° de la distribuci√≥n de los datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### - Patrones de compra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se analiza el comportamiento de los clientes en funci√≥n de sus transacciones realizadas considerando los productos comprados respecto a la semana de compra. Esto permite generar los ejemplos de compra que se utilizaran como clases postivas, a la vez que se generan ejemplos negativos para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_items_per_client = df_completed_true.groupby(['customer_id', 'product_id', 'purchase_date'])['items'].sum().reset_index()\n",
    "weekly_items_per_client['week'] = weekly_items_per_client['purchase_date'].dt.isocalendar().week\n",
    "weekly_items_per_client = weekly_items_per_client.groupby(['customer_id', 'product_id', 'week'])['items'].sum().reset_index() # tiene las compras por semana de cada cliente\n",
    "mean_weekly_items = weekly_items_per_client.groupby('customer_id')['items'].mean().reset_index()\n",
    "weekly_items_per_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafica\n",
    "global_mean = mean_weekly_items['items'].mean()\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.histplot(mean_weekly_items['items'], bins=200, kde=True)\n",
    "plt.title('Distribuci√≥n de compras semanales promedio por cliente. global mean: {:.2f}'.format(global_mean))\n",
    "plt.xlabel('Promedio de compras semanales por cliente')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_items_per_client['target'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera data sint√©tica que representa patrones de no compra de productos, considerando la cantidad de productos comprados por cliente en la semana de compra. Este bloque de c√≥digo genera ejemplos negativos (sin compra) por semana y cliente, tomando solo productos que el cliente s√≠ compr√≥ en semanas anteriores pero no compr√≥ en la semana actual. As√≠ se construyen observaciones con items = 0 y target = 0, que se agregan a las compras reales (target = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sint√©tica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = []\n",
    "semanas = sorted(weekly_items_per_client['week'].unique())\n",
    "\n",
    "# Procesamos semana por semana\n",
    "for w in semanas:\n",
    "    df_semana = weekly_items_per_client[weekly_items_per_client['week'] == w]\n",
    "    clientes_en_semana = df_semana['customer_id'].unique()\n",
    "    \n",
    "    for cid in clientes_en_semana:\n",
    "        productos_historicos = weekly_items_per_client[\n",
    "            (weekly_items_per_client['customer_id'] == cid) &\n",
    "            (weekly_items_per_client['week'] < w)\n",
    "        ]['product_id'].unique()\n",
    "        \n",
    "        if len(productos_historicos) == 0:\n",
    "            continue  # saltar si no hay historial\n",
    "        \n",
    "        productos_actuales = df_semana[df_semana['customer_id'] == cid]['product_id'].unique()\n",
    "        productos_no_comprados = set(productos_historicos) - set(productos_actuales)\n",
    "        \n",
    "        for pid in productos_no_comprados:\n",
    "            synthetic_data.append({\n",
    "                'customer_id': cid,\n",
    "                'product_id': pid,\n",
    "                'week': w,\n",
    "                'items': 0,\n",
    "                'target': 0,\n",
    "                'purchase_date': pd.to_datetime(\"2024-01-01\") + pd.to_timedelta((w - 1) * 7, unit='D')\n",
    "            })\n",
    "\n",
    "# remove week\n",
    "df_synthetic = pd.DataFrame(synthetic_data)\n",
    "df_synthetic = df_synthetic.drop(columns=['week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinar los foreign key como se hizo con df_completed_true\n",
    "df_completed_false = pd.merge(\n",
    "    df_synthetic,\n",
    "    df_productos_cleaned,\n",
    "    left_on='product_id',\n",
    "    right_on='product_id',\n",
    "    how='left'\n",
    ")\n",
    "df_completed_false = df_completed_false.merge(\n",
    "    df_clientes_cleaned,\n",
    "    left_on='customer_id',\n",
    "    right_on='customer_id',\n",
    "    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed_true.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed_false.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed_true['target'] = 1\n",
    "df_completed_false['target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea el dataset completo\n",
    "df_completed = pd.concat([df_completed_true, df_completed_false])\n",
    "\n",
    "# se elima la col order_id\n",
    "df_completed = df_completed.drop(columns=['order_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cantidad de transacciones por cliente\n",
    "transacciones_por_cliente = df_completed_true.groupby('customer_id').size().reset_index(name='num_transacciones')\n",
    "transacciones_por_cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transacciones por cliente dataset de transacciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "global_mean = transacciones_por_cliente['num_transacciones'].mean()\n",
    "globa_min, global_max = transacciones_por_cliente['num_transacciones'].min(), transacciones_por_cliente['num_transacciones'].max()\n",
    "sns.histplot(transacciones_por_cliente['num_transacciones'], bins='auto', kde=True)\n",
    "plt.title('Distribuci√≥n de transacciones por cliente. global mean: {:.2f}, min: {:.0f}, max: {:.0f}'.format(global_mean, globa_min, global_max))\n",
    "plt.xlabel('N√∫mero de transacciones por cliente')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar fechas ordenadas\n",
    "df_sorted = df_completed_true.sort_values(by=['product_id', 'customer_id', 'purchase_date'])\n",
    "\n",
    "# Calcular diferencias entre compras por producto-cliente\n",
    "df_sorted['days_between'] = (\n",
    "    df_sorted.groupby(['product_id', 'customer_id'])['purchase_date']\n",
    "    .diff()\n",
    "    .dt.days\n",
    ")\n",
    "\n",
    "# Calcular periodo promedio por SKU (product_id)\n",
    "recompra_promedio_sku = (\n",
    "    df_sorted.groupby('product_id')['days_between']\n",
    "    .mean()\n",
    "    .reset_index(name='avg_repurchase_days')\n",
    ")\n",
    "\n",
    "recompra_promedio_sku.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafica de barras. no histograma\n",
    "plt.figure(figsize=(25, 5))\n",
    "global_mean = recompra_promedio_sku['avg_repurchase_days'].mean()\n",
    "global_min, global_max = recompra_promedio_sku['avg_repurchase_days'].min(), recompra_promedio_sku['avg_repurchase_days'].max()\n",
    "sns.barplot(data=recompra_promedio_sku, x='product_id', y='avg_repurchase_days')\n",
    "plt.title(f'Promedio de d√≠as entre compras por SKU (product_id). global mean: {global_mean:.2f}, min: {global_min:.0f}, max: {global_max:.0f}')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('SKU')\n",
    "plt.ylabel('Promedio de d√≠as entre compras')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "el dataframe `mean_Weekly_items` entrega el promedio de compra de productos para cada cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¬øCu√°ntos productos compra en promedio cada cliente semana a semana?\n",
    "\n",
    "    En el dataframe `mean_weekly_items` se extrae cuando productos (cantidad de items) compra cada cliente en promedio por cada semana. Se observa una distribuci√≥n decreciente en la cantidad de productos al consdierar a todos los clientes, con un promedio entre todos los clientes de aproximadamente 19 productos, por semana.\n",
    "\n",
    "- ¬øCu√°ntas transacciones ha realizado cada cliente? \n",
    "\n",
    "    En el dataframe `transacciones_por_cliente` se resumen cuantos registros se tienen desde la tabla `df_transacciones` por cada customer_id. Entre todos los clientes se observa un promedio de 170 registros de compra, con extremos con clientes con solo un registro de compra y otro con 1600 registros de compra durante el periodo de estudio.\n",
    "\n",
    "\n",
    "- ¬øCu√°l es el periodo de recompra promedio de cada SKU?\n",
    "\n",
    "    Para el periodo de recompra se calcula el peiodo de tiempo en que se demora un cliente en recomprar un mismo producto. Luego para un mismo producto se calcula el promedio entre todos los periodos de recompra considernado todos los clientes que realizaron una recompra. Los resultados indican que en promedio un producto es recomprado cada 24 d√≠as con extremos de productos que son comprados de forma diar√≠a (sin periodo de recompra) y un producto que se recompra en 132 d√≠as\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Holdout [0.25 puntos]\n",
    "\n",
    "Para evaluar correctamente el modelo y garantizar su capacidad de generalizaci√≥n, se deben dividir los datos en tres conjuntos: \n",
    "- `Entrenamiento` : Para ajustar los par√°metros.\n",
    "- `Validaci√≥n`: Para optimizar hiperpar√°metros y seleccionar el mejor modelo.\n",
    "- `Prueba` : Para evaluar el rendimiento final en datos no vistos.\n",
    "\n",
    "üëÄ **Hint**: *Recuerde que los datos tienen una temporalidad que debe considerarse al momento de separarlos, para evitar fugas de informaci√≥n. Es importante justificar la estrategia de partici√≥n elegida y visualizar la distribuci√≥n temporal de los conjuntos generados*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos nuestro objetivo: construir un modelo predictivo que, cada semana, pueda estimar la probabilidad de compra de cada producto del portafolio para cada cliente activo. La idea es, una vez construido el modelo, darle esta informaci√≥n al equipo de Marketing para que aprovechen negocios nuevos u ofertas a clientes. Enm base a esto, escogeremos el modelo m√°s adecuado:\n",
    "\n",
    "Para esto, fijaremos una semilla para controlar la aleatoriedad y separaremos los datos en conjuntos de train (70%), validaci√≥n (20%) y prueba (10%) aproximadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_temporal(df, target_col, test_size=0.1, val_size=0.2, sort_by='purchase_date', seed=1):\n",
    "    df_sorted = df.sort_values(by=sort_by)\n",
    "    X = df_sorted.drop(columns=[target_col])\n",
    "    y = df_sorted[target_col]\n",
    "\n",
    "    # Separar test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_size, random_state=seed, shuffle=False)\n",
    "\n",
    "    # Separar validation del restante\n",
    "    val_prop = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_prop, random_state=seed, shuffle=False)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_temporal(\n",
    "    df_completed,\n",
    "    target_col='target',\n",
    "    test_size=0.1,\n",
    "    val_size=0.2,\n",
    "    sort_by='purchase_date',\n",
    "    seed=10\n",
    ")\n",
    "\n",
    "print(f\"Tama√±o Total: {df_completed.shape[0]}\")\n",
    "print(f\"Tama√±o Entrenamiento: {X_train.shape[0]}, representa el {int(X_train.shape[0]/df_completed.shape[0]*100)}%\")\n",
    "print(f\"Tama√±o X_train['purchase_date']Validaci√≥n: {X_val.shape[0]}, representa el {int(X_val.shape[0]/df_completed.shape[0]*100)}%\")\n",
    "print(f\"Tama√±o Prueba: {X_test.shape[0]}, representa el {int(X_test.shape[0]/df_completed.shape[0]*100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener rangos por split\n",
    "train_range = (X_train['purchase_date'].min(), X_train['purchase_date'].max())\n",
    "val_range = (X_val['purchase_date'].min(), X_val['purchase_date'].max())\n",
    "test_range = (X_test['purchase_date'].min(), X_test['purchase_date'].max())\n",
    "\n",
    "# Mostrar\n",
    "print(f\"Train: {train_range}\")\n",
    "print(f\"Val:   {val_range}\")\n",
    "print(f\"Test:  {test_range}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_date_ranges(train_range, val_range, test_range):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot([train_range[0], train_range[1]], [1, 1], label='Train', color='blue', linewidth=5)\n",
    "    plt.plot([val_range[0], val_range[1]], [2, 2], label='Validation', color='orange', linewidth=5)\n",
    "    plt.plot([test_range[0], test_range[1]], [3, 3], label='Test', color='green', linewidth=5)\n",
    "    plt.yticks([1, 2, 3], ['Train', 'Validation', 'Test'])\n",
    "    plt.title('Rangos de fechas por conjunto de datos')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_date_ranges(train_range, val_range, test_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Feature Engineering [0.5 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://i.imgur.com/CmXZSSC.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n, se deben construir pipelines para automatizar el preprocesamiento de los datos, lo cual garantizar√° que el flujo de trabajo sea reproducible y eficiente para esta entrega y las futuras. El objetivo es aplicar una serie de transformaciones en un orden definido para asegurar que los datos est√©n listos para los modelos a entrenar. El pipeline final debe incluir las t√©cnicas de pre-procesamiento que se deben aplicar a los distintos datos (seg√∫n lo que consideren necesario para el problema). Por ejemplo:\n",
    "\n",
    "- **Imputaci√≥n de valores nulos**: Manejo de datos faltantes mediante estrategias adecuadas (media, mediana, moda, interpolaci√≥n, etc.). \n",
    "\n",
    "- **Transformaciones personalizadas**: Uso de ColumnTransformer para aplicar diferentes transformaciones a columnas espec√≠ficas.\n",
    "\n",
    "- **Codificaci√≥n de variables categ√≥ricas**: Convertir datos categ√≥ricos a un formato num√©rico adecuado (One-Hot Encoding, Label Encoding, etc.).\n",
    "\n",
    "- **Discretizaci√≥n de variables**: Conversi√≥n de variables num√©ricas continuas en categor√≠as si son relevantes para el desempe√±o del modelo a entrenar.\n",
    "\n",
    "- **Estandarizaci√≥n o normalizaci√≥n** : Ajustar la escala de los datos para mejorar el rendimiento de los algoritmos sensibles a la magnitud de las variables.\n",
    "\n",
    "- **Eliminaci√≥n o transformaci√≥n de valores at√≠picos**: Identificar y tratar con datos outliers para mejorar la robustez del modelo.\n",
    "\n",
    "- **Nuevas caracter√≠sticas** : Creaci√≥n de variables adicionales que puedan aportar informaci√≥n relevante al modelo.\n",
    "\n",
    "Cada una de estas transformaciones debe ser justificada en funci√≥n de su relevancia para el problema y los datos, y es importante evaluar su impacto en el rendimiento del modelo. Adem√°s, el pipeline debe ser flexible y modular para poder probar diferentes configuraciones de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregacion de variables\n",
    "\n",
    "# se agrega una nueva feature al dataframe df_completed que contiene la cantidad de productos comprados por cliente por semana\n",
    "df_completed_ = df_completed.copy()\n",
    "df_completed_['week'] = df_completed_['purchase_date'].dt.isocalendar().week\n",
    "df_completed_['weekly_items'] = df_completed_.groupby(['customer_id', 'week'])['items'].transform('sum')\n",
    "df_completed_.drop(columns=['week'], inplace=True) # se elimina la columna week\n",
    "\n",
    "# se agrega la cantidad de transacciones por cliente\n",
    "df_completed_['num_transacciones'] = df_completed_.groupby('customer_id')['items'].transform('count')\n",
    "\n",
    "# se agrega la cantidad de transacciones por producto\n",
    "df_sorted = df_completed_.sort_values(by=['product_id', 'purchase_date'])\n",
    "df_sorted['repurchase_days'] = (\n",
    "    df_sorted.groupby('product_id')['purchase_date']\n",
    "    .diff()\n",
    "    .dt.days\n",
    ")\n",
    "\n",
    "recompra_promedio_sku = (\n",
    "    df_sorted.groupby('product_id')['repurchase_days']\n",
    "    .mean()\n",
    "    .reset_index(name='avg_repurchase_days')\n",
    ")\n",
    "\n",
    "df_completed_['sku_repurchase_days'] = df_completed_['product_id'].map(recompra_promedio_sku.set_index('product_id')['avg_repurchase_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_completed = df_completed_.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_completed.drop(columns=['items', 'order_id', 'region_id'], inplace=True)\n",
    "df_completed.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAbe mencionar que a aprtir del EDA realizado en el apartado anterior ya se habian agregado tres variables que buscan resumir el comportamiento de los clientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_int_columns_bar_charts(df_completed, max_unique_values=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los registros disponibles en el dataset `df_completed` representan interacciones reales en las que un cliente compr√≥ un producto en una fecha espec√≠fica. Por lo tanto, estos datos constituyen observaciones positivas del comportamiento que se desea modelar. Este tipo de observaci√≥n es fundamental para entrenar modelos predictivos que estimen la probabilidad de compra futura. Sin embargo, para construir un conjunto de entrenamiento balanceado y realista, es necesario generar tambi√©n registros negativos (y = 0), que representen combinaciones cliente-producto-semana donde no hubo compra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los an√°lisis exploratorios previos se observ√≥ que las variables `num_visit_per_week`, `zone_id` y `region_id` presentan un √∫nico valor constante en todos los registros del dataset. Al no aportar variabilidad ni capacidad de discriminaci√≥n entre observaciones, estas columnas no contribuyen al aprendizaje del modelo y, por tanto, se eliminan del conjunto de datos.\n",
    "\n",
    "Cabe notar que si bien el enunciado pide incluir informaci√≥n geogrpafica (explicitamente region y zona) los datos entregados no aportan informaci√≥n de dicha variabilidad. Para capturar parte de la informaci√≥n geogr√°fica se usaran las variables `X` e `Y` que corresponden al par de coordenas del cliente por transacci√≥n.\n",
    "\n",
    "Las features `customer_id` y `order_id` son foreign_key que no aportan informaci√≥n al modelo. Son eliminadas igualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el Dataframe `clientes`, existe un tipo de datos categ√≥rico que se transforma su tipo para ser mas desciptivo. Existe un valor nulo, se agrega un INputador de datos considerando remplazar el valor nulo por la media. Como parte del preprocesado se utiliza un OneHotEncoder para la variable ``customer_type``, mientras que se normalizan las variables n√∫mericas usando StandardScaler() para ``num_deliver_per_week`` y MinMaxScaler para las otras variables que no siguen (aparentemente) una distribuci√≥n normal: ``X``, ``Y``, ``num_visir_per_week``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion auxilar para enriquecer con nuevas feautures. Por revisar dado que puede ser un posible leak de datos.\n",
    "def enrich_features(X):\n",
    "    X = X.copy()\n",
    "    X['purchase_date'] = pd.to_datetime(X['purchase_date'], errors='coerce')\n",
    "    X['purchase_week'] = X['purchase_date'].dt.isocalendar().week.astype('category')\n",
    "    X['weekly_items'] = X.groupby(['customer_id', 'purchase_week'])['items'].transform('sum')\n",
    "\n",
    "    # N√∫mero de transacciones por cliente\n",
    "    X['num_transacciones'] = X.groupby('customer_id')['items'].transform('count')\n",
    "\n",
    "    # D√≠as entre compras por SKU\n",
    "    df_sorted = X.sort_values(by=['product_id', 'purchase_date'])\n",
    "    df_sorted['repurchase_days'] = df_sorted.groupby('product_id')['purchase_date'].diff().dt.days\n",
    "\n",
    "    avg_repurchase = df_sorted.groupby('product_id')['repurchase_days'].mean().reset_index()\n",
    "    avg_repurchase.columns = ['product_id', 'sku_repurchase_days']\n",
    "\n",
    "    X = X.merge(avg_repurchase, on='product_id', how='left')\n",
    "    X.drop(columns=['purchase_date'], inplace=True)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "numerical_features = ['X', 'Y', 'num_deliver_per_week', 'num_transacciones', 'sku_repurchase_days']\n",
    "numerical_features = ['X', 'Y']\n",
    "categorical_features = ['category', 'sub_category', 'segment', 'brand', 'customer_type', 'package']\n",
    "categorical_features+= ['purchase_week']\n",
    "\n",
    "# feature_engineering = FunctionTransformer(enrich_features)\n",
    "\n",
    "# funcion personalizada para desagregar la feature purchase_date\n",
    "def extract_date_features(X):\n",
    "    X = X.copy()\n",
    "    X['date'] = pd.to_datetime(X['purchase_date'], format='%d/%m/%y')\n",
    "    X['purchase_week'] = X['purchase_date'].dt.isocalendar().week\n",
    "    # categoricla\n",
    "    X['purchase_week'] = X['purchase_week'].astype('category')\n",
    "    #eliminar la columna original\n",
    "    X = X.drop(columns=['purchase_date'])\n",
    "    X = X.drop(columns=['date'])\n",
    "    return X\n",
    "\n",
    "function_transformer = FunctionTransformer(extract_date_features)\n",
    "\n",
    "\n",
    "# Pipelines separados\n",
    "num_pipeline = Pipeline([\n",
    "    ('outlier', IQR(lambda_=5)),\n",
    "    ('inputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numerical_features),\n",
    "    ('cat', cat_pipeline, categorical_features),\n",
    "], remainder='drop',\n",
    "   verbose_feature_names_out=False\n",
    ").set_output(transform='pandas')\n",
    "\n",
    "# Pipeline final\n",
    "pipeline_preprocessing = Pipeline([\n",
    "    ('transformer', function_transformer),\n",
    "    ('preprocessing', preprocessor),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def confuse_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Matriz de Confusi√≥n')\n",
    "    plt.xlabel('Predicci√≥n')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el DataFrame ``transacciones``, los tipos de datos son consistentes con los valores ingresados, por lo que no es necesario realizar transformaciones adicionales de tipo. Al analizar la distribuci√≥n de los datos, se observa que la feauture ``Items`` tiene con $\\lambda = 1$ tiene el 18.36% de los datos fuera del rango intercuratilico (IQR), lo cual implica la presentaci√≥n de una cantidad de outliers significativos. Existen 8384 registros que han sido devueltos a la empresa que corresponden a un 3.28% de las ventas.\n",
    "\n",
    "Dado que la variable ``Items`` constituye la variable objetivo del modelo de predicci√≥n de demanda, no debe ser tratada como una variable descriptiva ni aplicar transformaciones que alteren su distribuci√≥n. En cuanto al preprocesamiento, la √∫nica variable transformada es ``purchase_date``. A trav√©s de un FunctionTransformer, se descompone esta variable en tres nuevas caracter√≠sticas: d√≠a, mes y a√±o. Estas nuevas columnas se almacenan como variables categ√≥ricas utilizando el tipo category de pandas y se transformar mediante un OneHotEncoder()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Baseline [0.25 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExN3lzeGFqZmU3NzJrZHllNjRmaHVzczJpZ29rdHdlMzVpZnQwNXo1diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/qAtZM2gvjWhPjmclZE/giphy.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta secci√≥n se debe construir el modelo m√°s sencillo posible que pueda resolver el problema planteado, conocido como **Modelo baseline**. Su prop√≥sito es servir como referencia para comparar el rendimiento de los modelos m√°s avanzados desarrollados en etapas posteriores.  \n",
    "\n",
    "Pasos requeridos:  \n",
    "- Implemente, entrene y eval√∫e un modelo b√°sico utilizando un pipeline.  \n",
    "- Aseg√∫rese de incluir en el pipeline las transformaciones del preprocesamiento realizadas previamente junto con un clasificador b√°sico.  \n",
    "- Eval√∫e el modelo y presente el informe de m√©tricas utilizando **`classification_report`**.  \n",
    "\n",
    "Documente claramente c√≥mo se cre√≥ el modelo, las decisiones tomadas y los resultados obtenidos. Este modelo ser√° la base comparativa en las secciones posteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Pipeline con DummyClassifier\n",
    "pipeline_dummy = Pipeline([\n",
    "    ('transformacion', pipeline_preprocessing),\n",
    "    ('clasificador', DummyClassifier(strategy='uniform'))  # o 'stratified', 'uniform', etc.\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "pipeline_dummy.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_val_pred_dummy = pipeline_dummy.predict(X_val)\n",
    "\n",
    "# Evaluar\n",
    "print(\"Evaluaci√≥n Dummy en validaci√≥n:\")\n",
    "print(classification_report(y_val, y_val_pred_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Elecci√≥n de modelo [0.75 puntos]\n",
    "\n",
    "En esta secci√≥n deben escoger un modelo que se adapte a las necesidades del negocio. Para esto, pruebe al menos 3 modelos y desarrolle los siguientes aspectos para cada uno:\n",
    "\n",
    "- **Estructura y diferencias entre los modelos**: Explicar brevemente cada uno y sus hip√©rpar√°metros de mayor importancia.\n",
    "- **Clasificadores recomendados**:\n",
    "  - `LogisticRegression`\n",
    "  - `KNeighborsClassifier`\n",
    "  - `DecisionTreeClassifier`\n",
    "  - `SVC`\n",
    "  - `RandomForestClassifier`\n",
    "  - `LightGBMClassifier` (del paquete `lightgbm`)\n",
    "  - `XGBClassifier` (del paquete `xgboost`)\n",
    "  - Otro (seg√∫n lo que se estime adecuado)\n",
    "  \n",
    "- **Evaluaci√≥n de resultados**: Se utilizar√° el **`classification_report`** para evaluar el rendimiento de cada modelo, destacando m√©tricas clave como precisi√≥n, recall y F1-score. **Importante: No optimicen hiperpar√°metros, la idea es hacer una selecci√≥n r√°pida del modelo.**\n",
    "\n",
    "**Nota:** Pueden ocupar mas de 1 **instancia** de modelo para resolver el problema (e.g: (modelo_1, grupo_1), (modelo_2, grupo_2), ...).\n",
    "  \n",
    "A continuaci√≥n, se deben responder las siguientes preguntas para evaluar el rendimiento de los modelos entrenados:\n",
    "\n",
    "1. ¬øHay alg√∫n clasificador que supere al modelo baseline?  \n",
    "2. ¬øCu√°l es el mejor clasificador entrenado y por qu√©?  \n",
    "3. ¬øQu√© factores hacen que el mejor clasificador sea superior a los otros?  \n",
    "4. En t√©rminos de `tiempo de entrenamiento`, ¬øQu√© modelo considera m√°s adecuado para experimentar con grillas de optimizaci√≥n?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 1. Integrar preprocesamiento + modelo\n",
    "pipeline_TreeClassifier = Pipeline([\n",
    "    ('transformacion', pipeline_preprocessing),\n",
    "    ('clasificador', DecisionTreeClassifier(max_depth=5, random_state=10))\n",
    "])\n",
    "\n",
    "# 2. Entrenar el modelo\n",
    "pipeline_TreeClassifier.fit(X_train, y_train)\n",
    "\n",
    "# 3. Predecir sobre validaci√≥n\n",
    "y_val_pred = pipeline_TreeClassifier.predict(X_val)\n",
    "\n",
    "# 4. Evaluar\n",
    "print(\"Evaluaci√≥n en validaci√≥n:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "y_test_pred = pipeline_TreeClassifier.predict(X_test)\n",
    "print(\"Matriz de confusi√≥n en test:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pipeline_SVC = Pipeline([\n",
    "    ('transformacion', pipeline_preprocessing),\n",
    "    ('clasificador', SVC(kernel='linear', random_state=10))\n",
    "])\n",
    "# Entrenar el modelo\n",
    "pipeline_SVC.fit(X_train, y_train)\n",
    "# Predecir sobre validaci√≥n\n",
    "y_val_pred = pipeline_SVC.predict(X_val)\n",
    "# Evaluar\n",
    "print(\"Evaluaci√≥n en validaci√≥n:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "y_test_pred = pipeline_SVC.predict(X_test)\n",
    "print(\"Matriz de confusi√≥n en test:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pipeline_randomforest = Pipeline([\n",
    "    ('transformacion', pipeline_preprocessing),\n",
    "    ('clasificador', RandomForestClassifier(n_estimators=100, random_state=10))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline_randomforest.fit(X_train, y_train)\n",
    "y_val_pred = pipeline_randomforest.predict(X_val)\n",
    "\n",
    "# Evaluar\n",
    "print(\"Evaluaci√≥n en validaci√≥n:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pipeline con KNN\n",
    "pipeline_knn = Pipeline([\n",
    "    ('transformacion', pipeline_preprocessing),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=3))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_test_pred_knn = pipeline_knn.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "print(\"Evaluaci√≥n en conjunto de test con KNN:\")\n",
    "print(classification_report(y_test, y_test_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pipeline_logistic = Pipeline([\n",
    "    ('transformacion', pipeline_preprocessing),\n",
    "    ('clasificador', LogisticRegression(max_iter=1000, random_state=10))\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline_logistic.fit(X_train, y_train)\n",
    "# Predecir\n",
    "y_val_pred = pipeline_logistic.predict(X_val)\n",
    "# Evaluar\n",
    "print(\"Evaluaci√≥n en validaci√≥n:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "y_test_pred = pipeline_logistic.predict(X_test)\n",
    "print(\"Matriz de confusi√≥n en test:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. ¬øHay alg√∫n clasificador que supere al modelo baseline?  \n",
    "2. ¬øCu√°l es el mejor clasificador entrenado y por qu√©?  \n",
    "3. ¬øQu√© factores hacen que el mejor clasificador sea superior a los otros?  \n",
    "4. En t√©rminos de `tiempo de entrenamiento`, ¬øQu√© modelo considera m√°s adecuado para experimentar con grillas de optimizaci√≥n?\n",
    "\n",
    "**RESPUESTA:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Optimizaci√≥n de Hiperpar√°metros [1.0 puntos]\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExcXJkNzdhYjlneHplaGpsbnVkdzh5dnY3Y2VyaTIzamszdGR1czJ2diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/2rqEdFfkMzXmo/giphy.gif\" width=\"300\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de su an√°lisis anterior, se debe proceder a optimizar el rendimiento del modelo seleccionado mediante la optimizaci√≥n de sus hiperpar√°metros. Para ello, se espera que implementen `Optuna` para optimizar no solo los hiperpar√°metros del modelo, sino tambi√©n los de los preprocesadores utilizados (por ejemplo, OneHot Encoding, Scalers, etc.).\n",
    "\n",
    "Al desarrollar este proceso, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
    "\n",
    "- ¬øQu√© m√©trica decidieron optimizar y por qu√©?\n",
    "\n",
    "- ¬øQu√© hiperpar√°metro tuvo un mayor impacto en el rendimiento de su modelo?\n",
    "\n",
    "- ¬øCu√°nto mejor√≥ el rendimiento del modelo despu√©s de la optimizaci√≥n de hiperpar√°metros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "    from sklearn.metrics import make_scorer, f1_score\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    # Hiperpar√°metros que queremos optimizar\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 30)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "    bootstrap = trial.suggest_categorical(\"bootstrap\", [True, False])\n",
    "\n",
    "    # Modelo\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        bootstrap=bootstrap,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Pipeline completo: transformaci√≥n + modelo\n",
    "    full_pipeline = Pipeline([\n",
    "        ('transformer', function_transformer),\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Validaci√≥n cruzada con StratifiedKFold\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(\n",
    "        full_pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=cv,\n",
    "        scoring=make_scorer(f1_score, average=\"macro\")\n",
    "    ).mean()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=3)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "final_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('transformer', function_transformer),\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', final_model)\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "y_pred = final_pipeline.predict(X_val)\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ¬øQu√© m√©trica decidieron optimizar y por qu√©?\n",
    "\n",
    "- ¬øQu√© hiperpar√°metro tuvo un mayor impacto en el rendimiento de su modelo?\n",
    "\n",
    "- ¬øCu√°nto mejor√≥ el rendimiento del modelo despu√©s de la optimizaci√≥n de hiperpar√°metros?\n",
    "\n",
    "**RESPUESTA:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Interpretabilidad [1.0 puntos]\n",
    "\n",
    "En esta secci√≥n, deben explicar el funcionamiento de su modelo utilizando las t√©cnicas de interpretabilidad vistas en clase, como `SHAP`. Se espera que sean capaces de descomponer las predicciones y evaluar la importancia de los atributos y las interacciones entre ellos, con el fin de obtener una comprensi√≥n m√°s profunda de c√≥mo el modelo toma decisiones. \n",
    "\n",
    "Al desarrollar esta parte, deber√°n responder las siguientes preguntas clave como m√≠nimo:\n",
    "\n",
    "- ¬øPodr√≠a explicar el funcionamiento de su modelo para una predicci√≥n en particular? Si es as√≠, proporcione al menos tres ejemplos espec√≠ficos, describiendo c√≥mo el modelo lleg√≥ a sus decisiones y qu√© factores fueron m√°s relevantes en cada caso.\n",
    "\n",
    "- ¬øQu√© atributo tiene una mayor importancia en la salida de su modelo? Analice si esto tiene sentido con el problema planteado y justifique la relevancia de dicho atributo en el contexto de las predicciones que se realizan.\n",
    "\n",
    "- ¬øExiste alguna interacci√≥n entre atributos que sea relevante para el modelo? Investigue si la combinaci√≥n de ciertos atributos tiene un impacto significativo en las predicciones y expl√≠quela en **detalle**.\n",
    "\n",
    "- ¬øPodr√≠a existir sesgo hacia alg√∫n atributo en particular? Reflexione sobre la posibilidad de que el modelo est√© favoreciendo ciertos atributos. Si es as√≠, ¬øcu√°l podr√≠a ser la causa y qu√© impacto podr√≠a tener esto en la predicci√≥n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_feature_importance(model_pipeline, X, y):\n",
    "    \"\"\"Muestra la importancia de caracter√≠sticas del modelo usando permutaciones.\"\"\"\n",
    "    result = permutation_importance(\n",
    "        model_pipeline, X, y,\n",
    "        n_repeats=10, random_state=42, scoring='f1_macro'\n",
    "    )\n",
    "\n",
    "    importances = pd.Series(result.importances_mean, index=X.columns)\n",
    "    importances.sort_values().plot(kind='barh', figsize=(10,6), title='Importancia por permutaci√≥n')\n",
    "    plt.xlabel('Impacto en F1 macro')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def explain_with_shap(model_pipeline, X_sample):\n",
    "    \"\"\"Calcula valores SHAP para un pipeline con RandomForest.\"\"\"\n",
    "    # Extraer el modelo y el preprocesamiento\n",
    "    preprocessor = model_pipeline.named_steps['preprocessing']\n",
    "    model = model_pipeline.named_steps['model']\n",
    "    \n",
    "    # Aplicar transformaciones\n",
    "    X_transformed = preprocessor.transform(X_sample)\n",
    "\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_transformed)\n",
    "\n",
    "    # Visualizar para la primera muestra\n",
    "    shap.plots.waterfall(shap_values[0])\n",
    "    # Visualizaci√≥n global (opcional)\n",
    "    shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_feature_importance(final_pipeline, X_val, y_val)function_transformer\n",
    "# explain_with_shap(final_pipeline, X_val.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Resultados y Conclusiones [1.0 puntos]\n",
    "\n",
    "Para finalizar, se deben explicar los desarrollos y resultados obtenidos a lo largo de todo el proceso, desde la selecci√≥n de las variables hasta la optimizaci√≥n de hiperpar√°metros e interpretaci√≥n. Se espera una reflexi√≥n cr√≠tica sobre el desempe√±o de los modelos entrenados y una comparaci√≥n entre los diferentes enfoques. Adem√°s, deber√°n abordar los siguientes puntos clave:\n",
    "\n",
    "- **An√°lisis de m√©tricas**: Comenten sobre las m√©tricas obtenidas en cada etapa del modelo, destacando las m√°s relevantes como precisi√≥n, recall, F1-score, etc. ¬øCu√°les fueron los modelos m√°s efectivos? ¬øQu√© diferencias notables encontr√≥ entre ellos?\n",
    "\n",
    "- **Impacto de las decisiones tomadas**: Reflexionen sobre c√≥mo las decisiones relacionadas con el preprocesamiento, selecci√≥n de atributos y optimizaci√≥n de hiperpar√°metros influyeron en los resultados finales. ¬øHubo alguna decisi√≥n que haya tenido un impacto notable en el rendimiento?\n",
    "\n",
    "- **Lecciones aprendidas**: Concluyan sobre las lecciones m√°s importantes que aprendieron durante el proceso y c√≥mo estas pueden influir en futuras iteraciones del modelo. ¬øQu√© se podr√≠a mejorar si se repitiera el proceso? Si tuvieran m√°s recursos y tiempo, ¬øqu√© otras t√©cnicas/herramientas habr√≠an utilizado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **An√°lisis de m√©tricas**: Comenten sobre las m√©tricas obtenidas en cada etapa del modelo, destacando las m√°s relevantes como precisi√≥n, recall, F1-score, etc. ¬øCu√°les fueron los modelos m√°s efectivos? ¬øQu√© diferencias notables encontr√≥ entre ellos?\n",
    "\n",
    "    Las m√©tricas del modelo identifican una version sobreajustada o con posible leak de datos. El modelo m√°s efectivo fue el optimizado con optiuna. \n",
    " #TODO: MEJORAR ESTO\n",
    " \n",
    " **Impacto de las decisiones tomadas**: Reflexionen sobre c√≥mo las decisiones relacionadas con el preprocesamiento, selecci√≥n de atributos y optimizaci√≥n de hiperpar√°metros influyeron en los resultados finales. ¬øHubo alguna decisi√≥n que haya tenido un impacto notable en el rendimiento?\n",
    "\n",
    " **Respuesta**: La decisi√≥n de eliminar las variables `region_id`, `zone_id` y `customer_id` tuvo un impacto significativo en el rendimiento del modelo. Al eliminar estas variables, se redujo la complejidad del modelo y se mejor√≥ la interpretabilidad. Adem√°s, la elecci√≥n de utilizar OneHot Encoding para la variable `customer_type` permiti√≥ capturar la variabilidad entre los diferentes tipos de clientes, lo que result√≥ en un aumento en el rendimiento del modelo. Sin embargo, los resultados muestran un modelo sobreajustado u existe algun leak de datos que esta inflando las metricas.\n",
    "\n",
    " Por mejorar: Creemos que el enfoque si bien me permite predecir la probabilidad de que un cliente compre un producto, no es el planteamiento ams adecuado para resolver el problema de predecir sobre una semana en particular. En √∫ltima instancia, se nos ocurre que podemos que dado que estoy en la semana k y busco predecir la semana k+1, deberia entrenar el modelo con una ventana movil de las ultimas k-5 semanas. Esto ayudaria a caputar la temporalidad de los datos y la estacionalidad de los productos.\n",
    "\n",
    " #TODO: se cree que el modelo propuesto no resuelve el¬øn completitud el problema planteado, aunque si es una primera aproximaci√≥n.\n",
    "\n",
    "\n",
    "\n",
    "- **Lecciones aprendidas**: Concluyan sobre las lecciones m√°s importantes que aprendieron durante el proceso y c√≥mo estas pueden influir en futuras iteraciones del modelo. ¬øQu√© se podr√≠a mejorar si se repitiera el proceso? Si tuvieran m√°s recursos y tiempo, ¬øqu√© otras t√©cnicas/herramientas habr√≠an utilizado?\n",
    "\n",
    "**Respuesta**: Independiente de las m√©tricas resultantes, apesar que el planteamiento de la soluci√≥n no fue el m√°s adecuado, se aprendi√≥ que el uso de pipelines y la automatizaci√≥n del preprocesamiento de datos son herramientas valiosas para mejorar la eficiencia y reproducibilidad del trabajo. Adem√°s, la importancia de la interpretabilidad en los modelos de machine learning es fundamental para comprender c√≥mo funcionan y c√≥mo se pueden aplicar en el mundo real. En futuras iteraciones, ser√≠a interesante explorar t√©cnicas avanzadas de optimizaci√≥n de hiperpar√°metros y considerar el uso de modelos m√°s complejos como redes neuronales o modelos de ensemble para mejorar a√∫n m√°s el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mucho √©xito!\n",
    "\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExaHpvOTY5Z3hpdHI3aDBpdGRueXRqamZncXp2emFrbjJ5M2s5eTR1dSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/1PMVNNKVIL8Ig/giphy.gif\" width=\"300\" height=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labmds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
