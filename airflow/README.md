# Airflow DAG - MODEL_DAG

## DescripciÃ³n

El DAG principal de este proyecto, llamado **MODEL_DAG**, coordina todo el flujo de procesamiento y entrenamiento del modelo de predicciÃ³n de compras semanales.

## Tareas del Pipeline

### ğŸš€ `start`
- **Tipo**: EmptyOperator
- **PropÃ³sito**: Marca el inicio del flujo de trabajo

### ğŸ“ `create_folders`
- **FunciÃ³n**: Crea la estructura de carpetas necesarias
- **OrganizaciÃ³n**: Guarda datos por fecha de ejecuciÃ³n
- **Estructura**:
  - `/runs/FECHA/raw_data/` - Datos sin procesar
  - `/runs/FECHA/preprocessed_data/` - Datos preprocesados
  - `/runs/FECHA/splits_data/` - Conjuntos train/val/test
  - `/runs/FECHA/models/` - Modelos entrenados

### ğŸ“¥ `ingestion_data`
- **FunciÃ³n**: Carga los datasets originales
- **Datasets**: 
  - `clientes.parquet`
  - `productos.parquet` 
  - `transacciones.parquet`
- **Salida**: Rutas almacenadas en XCom para uso posterior

### ğŸ”§ `preprocessing_data`
- **FunciÃ³n**: Aplica limpieza y transformaciÃ³n de datos
- **Procesos**:
  - âœ… Limpia y balancea transacciones (`df_transacciones_clean`)
  - âœ… Elimina duplicados en clientes (`df_clientes_clean`)
  - âœ… Deduplica y mapea productos (`df_productos_clean`)
  - âœ… Genera dataset final fusionado (`df_completed.parquet`)
- **UbicaciÃ³n**: `/runs/FECHA/preprocessed_data/`

### ğŸ“Š `split_data`
- **FunciÃ³n**: DivisiÃ³n del dataset en conjuntos de entrenamiento
- **Salida**: Conjuntos train/validation/test listos para modelado
- **Formato**: Archivos parquet separados

### ğŸ¤– `train_model`
- **FunciÃ³n**: Entrenamiento del pipeline de Machine Learning
- **CaracterÃ­sticas**:
  - ğŸ”¹ Enriquecimiento de features (agregados por semana, cliente, producto)
  - ğŸ”¹ Preprocesamiento numÃ©rico y categÃ³rico
  - ğŸ”¹ Entrenamiento con RandomForest en Pipeline scikit-learn
- **Salida**: Modelo serializado en `/runs/FECHA/models/trained_model.pkl`

## Diagrama del Flujo

![Flujo del DAG](airflow.png)

El diagrama de flujo ilustra que el pipeline de entrenamiento es lineal. El DAG estÃ¡ configurado para cada 24 horas ejecutar el pipeline completo, asegurando que siempre se utilicen los datos mÃ¡s recientes y se guarde un nuevo modelo entrenado.

No se implementa el drift de datos.